{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a256bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d925870",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"scisumm.csv\",nrows = 201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a03f0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words(\"english\"))\n",
    "ownwords = [\"to\",\"the\",\"for\",\"fi\",\"ff\",\"zk\",\"x\",\"fifi\",\"f\",\"fl\",\"of\",\"for\",\"is\",\"are\",\"for\",\"to\",\"in\",\"and\",\"at\",\"with\",\"acc\",\"it\"]\n",
    "Prepositions = ['about','above','across','after','against','along','among','around','at','before','behind','between','beyond','but','by','concerning','despite','down','during','except','following','for','from','in','including','into','like','near','of','off','on','onto','out','over','past','plus','since','throughout','to','towards','under','until','up','upon','up','to','with','within','without']\n",
    "stopWords.update(ownwords)\n",
    "stopWords.update(Prepositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fb3ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextCleaning(text,num):\n",
    "    text = text.lower()\n",
    "    cleantext = re.sub(\"\\(.*?\\)\", '', text)\n",
    "    cleantext = re.sub(\"[0-9]\", '', cleantext)\n",
    "    cleantext = re.sub(\"(\\.\\.+)\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"(--+)\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"(~~+)\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"[<>()|&©ø\\[\\]\\'\\\";~*]\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"(\\+\\++)\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"(__+)\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"e.g.\", '',cleantext)\n",
    "    cleantext = re.sub(\"i.e.,\", '',cleantext)\n",
    "    cleantext = re.sub(\"acc.\", '',cleantext)\n",
    "    #cleantext = re.sub(\"[^a-zA-Z]\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"(\\s+)\",' ',cleantext)\n",
    "    if(num==0):\n",
    "        tokens = [w for w in cleantext.split() if not w in stopWords]\n",
    "    else:\n",
    "        tokens=cleantext.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>2:                                              \n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "511eaedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = []\n",
    "for t in data['text']:\n",
    "    cleaned_text.append(TextCleaning(t,2)) \n",
    "#call the function\n",
    "cleaned_summary = []\n",
    "for t in data['summary']:\n",
    "    cleaned_summary.append(TextCleaning(t,0))\n",
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary\n",
    "data.dropna(axis=0,inplace=True)\n",
    "print(type(data[\"cleaned_summary\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb0f1001",
   "metadata": {},
   "outputs": [],
   "source": [
    "allsum = []\n",
    "for text in data[\"cleaned_text\"]:\n",
    "    words = word_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentenceValue = dict()\n",
    "    for sentence in sentences:\n",
    "        for word, freq in freqTable.items():\n",
    "            if word in sentence.lower():\n",
    "                if sentence in sentenceValue:\n",
    "                    sentenceValue[sentence] += freq\n",
    "                else:\n",
    "                    sentenceValue[sentence] = freq\n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue:\n",
    "        sumValues += sentenceValue[sentence]\n",
    " \n",
    "    average = int(sumValues / len(sentenceValue))\n",
    "    summary = ''\n",
    "    sencont = 0\n",
    "    for sentence in sentences:\n",
    "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.6 * average)):\n",
    "            sencont+=1\n",
    "            summary += \" \" + sentence\n",
    "    allsum.append(summary)\n",
    "\n",
    "#data[\"Result1\"] = allsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "409deb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Result\"] = allsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "458837c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       additionally, present results the tagger the ...\n",
      "1       section define and compare five different con...\n",
      "2       describe statistical classifier that combines...\n",
      "3       given input sentence and target word frame, t...\n",
      "4       the potential benefit wide-coverage parsing w...\n",
      "                             ...                        \n",
      "196     they train from the penn treebank collection ...\n",
      "197     co-training weakly supervised paradigm for le...\n",
      "198     the unsupervised systems fact, all the seven ...\n",
      "199     japanese dependency structure analysis based ...\n",
      "200     syntactic reordering approaches are effective...\n",
      "Name: Result, Length: 201, dtype: object\n",
      "____________________\n",
      "0      tnt statistical part-of-speech tagger trigrams...\n",
      "1      mildly non-projective dependency structures sy...\n",
      "2      using corpus statistics wordnet relations sens...\n",
      "3      automatic labeling semantic roles present syst...\n",
      "4      generative models statistical parsing combinat...\n",
      "                             ...                        \n",
      "196    applying co-training methods statistical parsi...\n",
      "197    limitations co-training natural language learn...\n",
      "198    ish all-words task describe experience prepari...\n",
      "199    japanese dependency structure analysis based s...\n",
      "200    chinese syntactic reordering statistical machi...\n",
      "Name: cleaned_summary, Length: 201, dtype: object\n",
      "{'rouge1': Score(precision=0.4155844155844156, recall=0.4, fmeasure=0.40764331210191085), 'rouge2': Score(precision=0.13157894736842105, recall=0.12658227848101267, fmeasure=0.12903225806451615), 'rougeL': Score(precision=0.37662337662337664, recall=0.3625, fmeasure=0.36942675159235666)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference = data['cleaned_summary']\n",
    "hypotheses = data[\"Result\"]\n",
    "\n",
    "hypotheses = str(hypotheses)\n",
    "reference = str(reference)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(hypotheses,\n",
    "                      reference)\n",
    "print(hypotheses)\n",
    "print(\"____________________\")\n",
    "print(reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8b4f6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"scisumm.csv\",nrows = 201)\n",
    "cleaned_text = []\n",
    "for t in df['text']:\n",
    "    cleaned_text.append(TextCleaning(t,2)) \n",
    "#call the function\n",
    "cleaned_summary = []\n",
    "for t in df['summary']:\n",
    "    cleaned_summary.append(TextCleaning(t,0))\n",
    "df['cleaned_text']=cleaned_text\n",
    "df['cleaned_summary']=cleaned_summary\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a3b8e2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cll48\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stopwords(sen):\n",
    "        sen_new = \" \".join([i for i in sen if i not in stopWords])\n",
    "        return sen_new\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "allsum2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07284b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df[\"cleaned_text\"]:\n",
    "    sentences = []\n",
    "    sentences.append(sent_tokenize(text))\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    clean_sentences = pd.Series(sentences)\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    sentence_vectors = []\n",
    "    for i in clean_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "        \n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph,max_iter=10)\n",
    "\n",
    "    #Summary Extraction\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    # Extract top 10 sentences as the summary\n",
    "    summary2 = []\n",
    "    for i in range(4):\n",
    "        summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "    summary2 = str(summary2)\n",
    "    allsum2.append(summary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "067e3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.'], ['Parametric constraints With respect to the graded constraints, we find that multiplanarity is different from both gap degree and edge degree in that it involves a notion of optimization: since every dependency graph is m-planar for some sufficiently large m (put each edge onto a separate plane), the interesting question in the context of multiplanarity is about the minimal values for m that occur in real-world data.Since both planarity and well-nestedness are proper extensions of projectivity, we get the following hierarchy for sets of dependency graphs: projective C planar C well-nested C unrestricted The planarity constraint appears like a very natural one at first sight, as it expresses the intuition that ‘crossing edges are bad’, but still allows a limited form of non-projectivity.When we compare the two graded constraints to each other, we find that the gap degree measure partitions the data into less and larger clusters than the edge degree, which may be an advantage in the context of using the degree constraints as features in a data-driven approach towards parsing.The structural conditions we have presented here naturally fall into two groups: multiplanarity, gap degree and edge degree are parametric constraints with an infinite scale of possible values; planarity and well-nestedness come as binary constraints.However, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Nivre and Nilsson, 2005; Hall and Novák, 2005; McDonald and Pereira, 2006).In section 3, we define and compare five different constraints on mildly non-projective dependency structures that can be found in the literature: planarity, multiplanarity, well-nestedness, gap degree, and edge degree.'], \"['It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.']\", \"['Parametric constraints With respect to the graded constraints, we find that multiplanarity is different from both gap degree and edge degree in that it involves a notion of optimization: since every dependency graph is m-planar for some sufficiently large m (put each edge onto a separate plane), the interesting question in the context of multiplanarity is about the minimal values for m that occur in real-world data.Since both planarity and well-nestedness are proper extensions of projectivity, we get the following hierarchy for sets of dependency graphs: projective C planar C well-nested C unrestricted The planarity constraint appears like a very natural one at first sight, as it expresses the intuition that ‘crossing edges are bad’, but still allows a limited form of non-projectivity.When we compare the two graded constraints to each other, we find that the gap degree measure partitions the data into less and larger clusters than the edge degree, which may be an advantage in the context of using the degree constraints as features in a data-driven approach towards parsing.The structural conditions we have presented here naturally fall into two groups: multiplanarity, gap degree and edge degree are parametric constraints with an infinite scale of possible values; planarity and well-nestedness come as binary constraints.However, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Nivre and Nilsson, 2005; Hall and Novák, 2005; McDonald and Pereira, 2006).In section 3, we define and compare five different constraints on mildly non-projective dependency structures that can be found in the literature: planarity, multiplanarity, well-nestedness, gap degree, and edge degree.']\", '[\\'The next section presents one way in which the lexical knowledge in WordNet can be used to extract training examples automatically.\\', \"Little can be gained for a word like work, where the two senses, \\'activity\\' and \\'product,\\' are closely related and therefore difficult for the classifier to distinguish, due to a high degree of overlap in the training contexts.\", \\'Although these latter techniques are useful in their own right (e.g., spoken language systems or corrupted transmissions), the resulting materials do not generalize to the acquisition of tagged training for real polysemous or even homographic words.\\', \\'In the case of the noun line, topical was slightly better than local at all set sizes, but with 200 training examples, their combination yielded 84% accuracy, greater than either topical (78%) or local (67%) alone (see Figure 3).\\', \\'Testing consists of taking a new example of the polysemous word and computing the most probable sense, based on the cues present in the context of the new item.\\', \\'Section 2 describes a statistical classifier, TLC (Topical/Local Classifier), that uses topical context (the open-class words that co-occur with a particular sense), local context (the open- and closed-class items that occur within a small window around a word), or a combination of the two.\\']', \"['Using head word, phrase type, and target word without either position or grammatical function yielded only 76.3% accuracy, indicating that although the two features accomplish a similar goal, it is important to include some measure of the constituent’s relationship to the target word, whether relative position or either of the syntactic features.We first describe in detail the sentence- and constituent-level features on which our system is based and then use these features to calculate probabilities for predicting frame element labels in Section 4.2.The mapping of roles within a frame is generally one to one, and therefore the choice of mapping has little effect when using statistics conditioned on the target word and on the frame, as in the previous section.Separate searches were performed for various patterns over lexical items and part-of-speech sequences in the target words’ context, producing a set of subcorpora for each target word, designed to capture different argument structures and ensure that some examples of each possible syntactic usage of the target word would be included in the final database.In addition to the robustness of even relatively simple parsing models, one explanation for the modest improvement may be the fact that even our integrated system includes semantic information for only one word in the sentence.One technique for dealing with the sparseness of lexical statistics would be the combination of FrameNet data with named-entity systems for recognizing times, dates, and locations, the effort that has gone into recognizing these items, typically used as adjuncts, should complement the FrameNet data, which is more focused on arguments.']\", \"['The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.If P is a leaf node, generate its head word w. Otherwise, generate the category ofits head daughter H . If P is binary branching, gen erate the category of its non-head daughter S (a complement or modifier of H).The model itself includes no prior knowledge spe cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability).Therefore, we alsoevaluate performance using a dependency evaluation reported by Collins (1999), which counts word word dependencies as determined by local trees and their labels.This results in a smaller and less overgenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate gories in sections 02-21, instead of >12,400 in the original Treebank representation (Collins, 1999)).In the unlabeled case hi (where it only matters whether word a is a dependent of word b, not whatthe label of the local tree is which defines this depen dency), scores can be compared across grammars with different sets of labels and different kinds of trees.LikeCollins (1999), we assume that the test data is POS tagged, and can therefore replace unknown words inthe test data with their POS-tag, which is more ap propriate for a formalism like CCG with a large set of lexical categories than one generic token for all unknown words.']\", \"['I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern.I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern.One problem with the training methods given in section 2.3 is the restriction of training data to nouns in H. Many nouns, especially common ones, have verbal or adjectival usages that preclude them from being in N. Yet when they occur as nouns, they still provide useful training information that the current system ignores.To determine the difference made by conceptual association, the pattern training scheme has been retrained using lexical counts for both the dependency and adjacency model, but only for the words in the test set.Corpus Statistics Meet The Noun Compound: Some Empirical Results Tagged Dependency -.— Tagged Adjacency -e— L. Pattern 3 5 Training scheme (integers denote window widths) Figure 5: Accuracy using a tagged corpus for various training schemes data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus.It is reproduced here for reference: Given three nouns n1, n2 and n3: Only more recently has it been suggested that corpus statistics might provide the oracle, and this idea is the basis of the three algorithms which use the adjacency model.']\", \"['On the one hand, 325 Computational Linguistics Volume 19, Number 2 it is used to annotate what Kay has called permanent predictable ambiguities, allowing an annotator to indicate that a structure isglobally ambiguous even given the surrounding context (annotators always assign structure to a sentence on the basis of its context).We would like to emphasize that the lexical and syntactic recoverability inherent in the POS-tagged version of the Penn Treebank corpus allows end users to employ a much richer tagset han the small one described in Section 2.2 if the need arises.For example, an SBAR containing the word to immediately before the VP will necessarily be infinitival, while an SBAR containing a verb or auxiliary with a 10 We would like to emphasize that the percentage given for the modified output of PARTS does not represent an error rate for PARTS.To take another example, so-called that-clauses can be identified easily by searching for SBARs containing the word that or the null element 0 in initial position.We have chosen to retain these null elements because we believe that they can be exploited in many cases to establish a sentences predicate-argument structure; at least one recipient of the parsed corpus has used it to bootstrap the development of lexicons for partic- ular NLP projects and has found the presence of null elements to be a considerable aid in determining verb transitivity (Robert Ingria, personal communication).For a given text and n annotators, there are disagreement ratios (one for each possible pair of annotators).']\", '[\\'rank t, erms according 1o their -2lo9~ value 4. select a c(mfid(mce l , vel fiom the A;: (listril)utiotl table; (letermin(~ the cutotf associated weight, mid the numl)(n\" of t(nms to he included iIl the signatures 5 The Corpus The training data derives Kern the Question and Answering summary evahmtion data provided l)y T IPSTEI / .\\', \\'Althottgh many rel<want docttments are avaita})l+, for each t01>ic, Ollly SOlll0 o[ [h0111 htlv(~ allSWOl kc!y 499 markut)s. The mnnber of documents with answer keys are listed in the row labeled: \"# of Relevant Does Used in Training\".\\', \\'The Automated Acquisit ion of Topic Signatures for Text Summarizat ion Chin -Yew L in  and  Eduard  Hovy In fo rmat ion  S(:i(umes I l l s t i tu te Un ivers i ty  of Southern  Ca l i fo rn ia Mar ina  del Rey, CA  90292, USA { cyl,hovy }C~isi.edu Abst rac t In order to produce, a good summary, one has to identify the most relevant portions of a given text.\\', \\'4.1  S ignature  Term Ext rac t ion  and  Weight Es t imat ion ()n the assumption that semantically related terms tend to co-occur, on( can construct topic signa- tures flom preclassified text using the X 2 test, mu-.\\', \\'her of t, ernl occurrence, in the corpus, 7/(/~) is the entropy of terms over relevant and nonrelevant sets of documents, 7/ (  fe l t  ) is the entropy of a given term OVel\" relev;inL ~/nd nonl  ( .qeval l t  sets  of doel l inel lLS, ~tll(1 Z(R.; T) i:; the inutual information between docu- ment relevancy and a given t(.rm.\\', \\'We have documents l)[e.classitied into a :;(~t, \"R. of relevant exts and a set ~.\\']', \"['However, we expect that in real data, the differences between parallel and non-parallel pairs are less clear than in our test data (see the discussion in Section 7) and can no The amounts of data processed by our system during extraction from the Chinese-English comparable corpus.We also show that language pairs for which very little parallel data is available are likely to benefit the most from our method; by running our extraction system on a large comparable corpus in a bootstrapping manner, we can obtain performance improvements of more than 50% over a baseline MT system trained only on existing parallel data.After the article selection step, we simply paired each foreign document with the best-matching English one, assumed they are parallel, sentence-aligned them with a generic sentence alignment method, and added the resulting data to the training corpus.As we have shown, this could be particularly effective for language pairs for which only very small amounts of parallel data are available.The solution we propose is to get comparable news data, automatically extract parallel sentences from it, and use these sentences as additional training data; we will show that doing this improves translation performance on a news test set.The first type concerns cases when the system classifies as parallel sentence pairs that, although they share many content words, express slightly different meanings, as in Figure 15, example 7.']\", '[\\'At present, the apparent best source of sense distinctions is assumed to be on-line resources such as LDOCE or WordNet, although the problems of utilizing such resources are well known, and their use does not address issues of more complex semantic tagging that goes beyond the typical distinctions made in dictionaries and thesauri.\\', \"It is interesting to note that Weaver\\'s text also outlined the statistical approach to language analysis prevalent now, nearly fifty years later: This approach brings into the foreground an aspect of the matter that probably is absolutely basic—namely, the statistical character of the problem.... And it is one of the chief purposes of this memorandum to emphasize that statistical semantic studies should be undertaken, as a necessary primary step.\", \\'Now that other problems amenable to these methods, such as part-of-speech disambiguation and alignment of parallel translations, have been fairly thoroughly addressed, the problem of word sense disambiguation has taken center stage, and it is frequently cited as one of the most important problems in natural language processing research today.\\', \"Roget\\'s International Thesaurus, which was put into machine-tractable form in the 1950s and has been used in a variety of applications including machine translation (Masterman 1957), information retrieval (Sparck-Jones 1964, 1986), and content analysis (Sedelow and Sedelow [1969], see also Sedelow and Sedelow [1986, 1992]), also supplies an explicit concept hierarchy consisting of up to eight increasingly refined levels.8 Typically, each occurrence of the same word under different categories of the thesaurus represents different senses of that word; i.e., the categories correspond roughly to word senses (Yarowsky 1992).\", \\'However, to date there has been little systematic study Ide and Wronis Introduction of the contribution of different information types for different types of target words.\\', \\'Any of a variety of association methods is used to determine the best match between the current context and one of these sources of information, in order to assign a sense to each word occurrence.\\']']\n",
      "____________________\n",
      "0     tnt statistical part-of-speech tagger trigrams...\n",
      "1     mildly non-projective dependency structures sy...\n",
      "2     using corpus statistics wordnet relations sens...\n",
      "3     automatic labeling semantic roles present syst...\n",
      "4     generative models statistical parsing combinat...\n",
      "5     corpus statistics meet noun compound: empirica...\n",
      "6     building large annotated corpus ish: penn tree...\n",
      "7     automated acquisition topic signatures text su...\n",
      "8     improving machine translation performance expl...\n",
      "9     introduction special issue word sense disambig...\n",
      "10    providing unified unt definite noun phrases di...\n",
      "11    unsupervised learning narrative schemas partic...\n",
      "12    name tagging word clusters discriminative trai...\n",
      "13    effects adjective orientation gradability sent...\n",
      "14    minimized models unsupervised part-of-speech t...\n",
      "15    polynomial-time algorithm statistical machine ...\n",
      "16    unsupervised morpheme-based hmm hebrew morphol...\n",
      "17    importance supertagging wide-coverage ccg pars...\n",
      "18    overview bionlp shared task bionlp shared task...\n",
      "19    word-sense disambiguation using statistical me...\n",
      "20    machine translation using probabilistic synchr...\n",
      "21    joshua: open source toolkit parsing-based mach...\n",
      "22    incrementality deterministic dependency parsin...\n",
      "23    semeval- task tempeval temporal relation ident...\n",
      "24    procedure quantitatively comparing syntactic c...\n",
      "25    generating phrasal sentential paraphrases: sur...\n",
      "26    tree-to-string alignment template statistical ...\n",
      "27    confidence estimation machine translation pres...\n",
      "28    semeval- task cross-lingual textual entailment...\n",
      "29    pronunciation modeling improved spelling corre...\n",
      "30    unsupervised models named entity classificatio...\n",
      "31    distinguishing word senses untagged text paper...\n",
      "32    mixture-model adaptation smt describe mixture-...\n",
      "33    hierarchical phrase-based model statistical ma...\n",
      "34    cormet: computational corpus-based conventiona...\n",
      "35    polylingual topic models topic models useful t...\n",
      "36    multiple aspect ranking using thod grief algor...\n",
      "37    multi-pass sieve coreference resolution corefe...\n",
      "38    kappa statistic: second look recent years, kap...\n",
      "39    dependency treelet translation: syntactically ...\n",
      "40    classifier-based approach preposition determin...\n",
      "41    concept discovery text broad-coverage lexical ...\n",
      "42    thnerative lexicon paper, discuss four major t...\n",
      "43    bayesian learning tree substitution grammar tr...\n",
      "44    generative constituent-context model improved ...\n",
      "45    smorgasbord features statistical machine trans...\n",
      "46    fluency adequacy hter? exploring different hum...\n",
      "47    robust applied morphological generation practi...\n",
      "48    corpus-based approach building semantic lexico...\n",
      "49    automatically constructing lexicon verb phrase...\n",
      "Name: cleaned_summary, dtype: object\n",
      "{'rouge1': Score(precision=0.4941860465116279, recall=0.013968775677896467, fmeasure=0.027169570081508713), 'rouge2': Score(precision=0.0641399416909621, recall=0.0018078724628153506, fmeasure=0.003516624040920716), 'rougeL': Score(precision=0.1686046511627907, recall=0.004765817584223501, fmeasure=0.009269618027808854)}\n"
     ]
    }
   ],
   "source": [
    "reference = data['cleaned_summary'][0:50]\n",
    "hypotheses = allsum2[0:50]\n",
    "\n",
    "hypotheses = str(hypotheses)\n",
    "hypotheses = re.sub(\"', '\", '', hypotheses)\n",
    "reference = str(reference)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(hypotheses,\n",
    "                      reference)\n",
    "print(hypotheses)\n",
    "print(\"____________________\")\n",
    "print(reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d2176b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use context-independent approach for did for the contextual wts turned out good choice set all the standard deviation the unconditioned maximum likelihood probabilities the tags the training corpus, set for all using tagset tags and the average this usually yields values the range use different estimates for uppercase and lowercase words, maintain two different suffix tries depending the capitalization the word.', 'causes the probability complete sequence set zero its use necessary for new text sequence, thus makes impossible rank different sequences containing zero probability.', 'second, learning curves are presented, that indicate the performance when using training corpora different starting with few tokens and ranging the size the entire corpus important characteristic statistical taggers that they not only assign tags words but also probabilities order rank different assignments.', 'nevertheless, recent independent comparison taggers has shown that another approach even works better: markov models combined with good smoothing technique and with handling unknown words.']\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][0]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "print(summary2)\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][0]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fd185de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['use context-independent approach for did for the contextual wts turned out good choice set all the standard deviation the unconditioned maximum likelihood probabilities the tags the training corpus, set for all using tagset tags and the average this usually yields values the range use different estimates for uppercase and lowercase words, maintain two different suffix tries depending the capitalization the word.',\n",
       "  'causes the probability complete sequence set zero its use necessary for new text sequence, thus makes impossible rank different sequences containing zero probability.',\n",
       "  'second, learning curves are presented, that indicate the performance when using training corpora different starting with few tokens and ranging the size the entire corpus important characteristic statistical taggers that they not only assign tags words but also probabilities order rank different assignments.',\n",
       "  'nevertheless, recent independent comparison taggers has shown that another approach even works better: markov models combined with good smoothing technique and with handling unknown words.']]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allsum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96aa5248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['use context-independent approach for did for the contextual wts turned out good choice set all the standard deviation the unconditioned maximum likelihood probabilities the tags the training corpus, set for all using tagset tags and the average this usually yields values the range use different estimates for uppercase and lowercase words, maintain two different suffix tries depending the capitalization the word.causes the probability complete sequence set zero its use necessary for new text sequence, thus makes impossible rank different sequences containing zero probability.second, learning curves are presented, that indicate the performance when using training corpora different starting with few tokens and ranging the size the entire corpus important characteristic statistical taggers that they not only assign tags words but also probabilities order rank different assignments.nevertheless, recent independent comparison taggers has shown that another approach even works better: markov models combined with good smoothing technique and with handling unknown words.']]\n",
      "____________________\n",
      "tnt statistical part-of-speech tagger trigrams tags efficient statistical part-of-speech tagger. contrary claims found elsewhere literature, argue tagger based markov models performs least well current approaches, maximum entropy framework. recent comparison even shown tnt performs significantly better tested corpora. describe basic model tnt, techniques used smoothing handling unknown words. furthermore, present evaluations two corpora. achieve automated tagging syntactic-structure-based set grammatical function tags phrase-chunk syntactic-role modifiers trained supervised mode tree bank german.\n",
      "{'rouge1': Score(precision=0.34615384615384615, recall=0.17647058823529413, fmeasure=0.23376623376623376), 'rouge2': Score(precision=0.03896103896103896, recall=0.019736842105263157, fmeasure=0.026200873362445413), 'rougeL': Score(precision=0.15384615384615385, recall=0.0784313725490196, fmeasure=0.1038961038961039)}\n"
     ]
    }
   ],
   "source": [
    "reference1 = df['cleaned_summary'][0]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8e953758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emphasize the word above, because planarity understood here does not coincide with the standard graph-theoretic concept the same name, where one would allowed also use the area below the sentence disentangle the figure shows dependency graph that planar but not projective: while there are crossing the yield the node does not form interval.', 'when compare the two graded constraints each other, find that thp degree measure partitions the data into less and larger clusters than the degree, which may advantage the context using the degree constraints features data-driven approach towards parsing.', 'parametric constraints with respect thaded constraints, find that multiplanarity different from both gap degree and degree that involves notion optimization: since every dependency graph m-planar for some sufficiently large the interesting question the context multiplanarity about the minimal values for that occur real-world data.', 'however, recent results non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis natural language are very nearly projective, differing only minimally from the best projective approximation this raises the question whether possible characterize class mildly non-projective dependency structures that rich enough unt for naturally occurring syntactic constructions, yet restricted enough enable efficient parsing.']\n",
      "[['emphasize the word above, because planarity understood here does not coincide with the standard graph-theoretic concept the same name, where one would allowed also use the area below the sentence disentangle the figure shows dependency graph that planar but not projective: while there are crossing the yield the node does not form interval.when compare the two graded constraints each other, find that thp degree measure partitions the data into less and larger clusters than the degree, which may advantage the context using the degree constraints features data-driven approach towards parsing.parametric constraints with respect thaded constraints, find that multiplanarity different from both gap degree and degree that involves notion optimization: since every dependency graph m-planar for some sufficiently large the interesting question the context multiplanarity about the minimal values for that occur real-world data.however, recent results non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis natural language are very nearly projective, differing only minimally from the best projective approximation this raises the question whether possible characterize class mildly non-projective dependency structures that rich enough unt for naturally occurring syntactic constructions, yet restricted enough enable efficient parsing.']]\n",
      "____________________\n",
      "mildly non-projective dependency structures syntactic parsing requires fine balance expressivity complexity, naturally occurring structures rately parsed compromising efficiency. dependency-based parsing, several constraints proposed restrict class permissible structures, projectivity, planarity, multi-planarity, well-nestedness, gap degree, degree. projectivity generally taken restrictive natural language syntax, clear proposals strikes best balance expressivity complexity. paper, review compare different constraints theoretically, provide experimental evaluation using data two treebanks, investigating large proportion structures found treebanks permitted different constraints. results indicate combination well-nestedness constraint parametric constraint discontinuity gives good fit linguistic data.\n",
      "{'rouge1': Score(precision=0.48863636363636365, recall=0.21182266009852216, fmeasure=0.29553264604810997), 'rouge2': Score(precision=0.10344827586206896, recall=0.04455445544554455, fmeasure=0.06228373702422146), 'rougeL': Score(precision=0.125, recall=0.054187192118226604, fmeasure=0.07560137457044674)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][1]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "print(summary2)\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][1]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b71c7559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leacock, towel!, and voorhees found that the response patterns the three classifiers convd, suggesting that each the classifiers was extracting much data available purely topical approaches that look only word counts from training examples.', 'leacock, towell, and voorhees showed that performance the content vector topical classifier could improved with the addition local templates— specific word patterns that were recognized being indicative particular sense— extension idea initially suggested weiss although the templates proved highly reliable when they occurred, all too often, none were found.', 'summary, the results obtained with tlc support the following preliminary conclusions: improvement with training levels off after about training examples for the least frequent sense the high predictive power local context for the verb and adjective indicate that the local parameters effectively capture syntactically mediated relations, the subject and object complement verbs, the noun that adjective modifies nouns may more quot topical quot than verbs and adject and therefore benefit more from the combination topical and local context the precision tlc can considerably improved the price recall, trade-off that may desirable some interactive nlp applications.', 'testing consists taking new example the polysemous word and computing the most probable sense, based the cues present the context the new item.']\n",
      "[['leacock, towel!, and voorhees found that the response patterns the three classifiers convd, suggesting that each the classifiers was extracting much data available purely topical approaches that look only word counts from training examples.leacock, towell, and voorhees showed that performance the content vector topical classifier could improved with the addition local templates— specific word patterns that were recognized being indicative particular sense— extension idea initially suggested weiss although the templates proved highly reliable when they occurred, all too often, none were found.summary, the results obtained with tlc support the following preliminary conclusions: improvement with training levels off after about training examples for the least frequent sense the high predictive power local context for the verb and adjective indicate that the local parameters effectively capture syntactically mediated relations, the subject and object complement verbs, the noun that adjective modifies nouns may more quot topical quot than verbs and adject and therefore benefit more from the combination topical and local context the precision tlc can considerably improved the price recall, trade-off that may desirable some interactive nlp applications.testing consists taking new example the polysemous word and computing the most probable sense, based the cues present the context the new item.']]\n",
      "____________________\n",
      "using corpus statistics wordnet relations sense identification corpus-based approaches word sense identification flexibility generality suffer knowl acquisition bottleneck. show knowl-based techniques used open bottleneck automatically locating training corpora. describe statistical classifier combines topical context local cues identity word sense. classifier used disambiguate noun, verb, adjective. knowl base form wordnet lexical relations used automatically locate training examples general text corpus. test results compared manually tagged training examples. present method obtain sense-tagged examples using monosemous relatives.\n",
      "{'rouge1': Score(precision=0.37662337662337664, recall=0.14356435643564355, fmeasure=0.2078853046594982), 'rouge2': Score(precision=0.039473684210526314, recall=0.014925373134328358, fmeasure=0.021660649819494584), 'rougeL': Score(precision=0.16883116883116883, recall=0.06435643564356436, fmeasure=0.0931899641577061)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][2]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "print(summary2)\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][2]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "82a4aa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the first step apply bayes’ rule this and the second make the assumption that the features the various constituents sentence are independent given the target word and each constituent’s role and discard the term which constant with respect estimate the prior over frame element assignments the probability the frame element groups, represented with the set operator {}: and finally discard the feature prior being constant over the argmax expression: this leaves with expression terms the prior for frame element groups particular target word the local probability frame element given constituent’s features which our previous system was based, and the individual priors for the frame elements chosen this formulation can used assign roles either when the frame element boundaries are known when they are not, will discuss later this section.', 'using head word, phrase type, and target word without either position grammatical function yielded only racy, indicating that although the two features mplish similar goal, important include some measure the constituent’s relationship the target word, whether relative position either the syntactic features.', 'the mapping roles within frame generally one one, and therefore the choice mapping has little effect when using statistics conditioned the target word and the frame, the previous section.', 'addition the robustness even relatively simple parsing models, one explanation for the modest improvement may the fact that even our integrated system includes semantic information for only one word the sentence.']\n",
      "[['the first step apply bayes’ rule this and the second make the assumption that the features the various constituents sentence are independent given the target word and each constituent’s role and discard the term which constant with respect estimate the prior over frame element assignments the probability the frame element groups, represented with the set operator {}: and finally discard the feature prior being constant over the argmax expression: this leaves with expression terms the prior for frame element groups particular target word the local probability frame element given constituent’s features which our previous system was based, and the individual priors for the frame elements chosen this formulation can used assign roles either when the frame element boundaries are known when they are not, will discuss later this section.using head word, phrase type, and target word without either position grammatical function yielded only racy, indicating that although the two features mplish similar goal, important include some measure the constituent’s relationship the target word, whether relative position either the syntactic features.the mapping roles within frame generally one one, and therefore the choice mapping has little effect when using statistics conditioned the target word and the frame, the previous section.addition the robustness even relatively simple parsing models, one explanation for the modest improvement may the fact that even our integrated system includes semantic information for only one word the sentence.']]\n",
      "____________________\n",
      "automatic labeling semantic roles present system identifying semantic relationships, semantic roles, filled constituents sentence semantic frame. given input sentence target word frame, system labels constituents either abstract semantic roles, agent patient, domain-specific semantic roles, speaker, message, topic. system based statistical classifiers trained roughly sentences hand-annotated semantic roles framenet semantic labeling project. parsed training sentence syntactic tree extracted various lexical syntactic features, phrase type constituent, grammatical function, position sentence. features combined knowl predicate verb, noun, adjective, well information prior probabilities various combinations semantic roles. used various lexical clustering algorithms generalize possible fillers roles. test sentences parsed, annotated features, passed classifiers. system achieves racy identifying semantic role presegmented constituents. difficult task simultaneously segmenting constituents identifying semantic role, system achieved precision recall. study also allowed compare usefulness different features feature combination methods semantic role labeling task. also explore integration role labeling statistical syntactic parsing attempt generalize predicates unseen training data. propose first srl model framenet.\n",
      "{'rouge1': Score(precision=0.2857142857142857, recall=0.1888412017167382, fmeasure=0.22739018087855295), 'rouge2': Score(precision=0.032679738562091505, recall=0.021551724137931036, fmeasure=0.025974025974025976), 'rougeL': Score(precision=0.14285714285714285, recall=0.0944206008583691, fmeasure=0.11369509043927647)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][3]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "print(summary2)\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][3]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b2e17efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for further discussion refer the reader clark and hockenmaier ccgbank corpus ccg normal-form derivations obtained translating the penn tree bank trees using algorithm described hockenmaier and steedman almost alltypes construction?with the exception gap ping and ucp arecovered the translation procedure, which pro cesses the sentences the training corpus and the sentences the test corpus thammar contains set type-changing rules similar the lexical rules described carpenter figure shows derivation taken from ccgbank.', 'for all experiments reported here and section the frequency threshold was set likecollins assume that the test data pos tagged, and can therefore replace unknown words inthe test data with their pos-tag, which more propriate for formalism like ccg with large set lexical categories than onneric token for all unknown words.', 'this model was originally described hockenmaier where was applied preliminary version ccgbank, and its definition repeated here the top row table given node with category choose the expansion exp where exp can beleaf unary left right leaf node, generate its head word otherwise, generate the category ofits head daughter binary branching, gen erate the category its non-head daughter .the model itself includes prior knowl spe cific ccg other than that only allows unary andbinary branching trees, and that the sets nontermi nals and terminals are not disjoint all the experiments reported this section were conducted using sections ccgbank training corpus, and section test corpus.', 'the performance the baseline model shown the top row table for six out the sentences our test corpus not get parse.the reason that lexicon consisting the word category pairs observed the training corpus does not contain all the entries required parse the test corpus.']\n",
      "[['for further discussion refer the reader clark and hockenmaier ccgbank corpus ccg normal-form derivations obtained translating the penn tree bank trees using algorithm described hockenmaier and steedman almost alltypes construction?with the exception gap ping and ucp arecovered the translation procedure, which pro cesses the sentences the training corpus and the sentences the test corpus thammar contains set type-changing rules similar the lexical rules described carpenter figure shows derivation taken from ccgbank.for all experiments reported here and section the frequency threshold was set likecollins assume that the test data pos tagged, and can therefore replace unknown words inthe test data with their pos-tag, which more propriate for formalism like ccg with large set lexical categories than onneric token for all unknown words.this model was originally described hockenmaier where was applied preliminary version ccgbank, and its definition repeated here the top row table given node with category choose the expansion exp where exp can beleaf unary left right leaf node, generate its head word otherwise, generate the category ofits head daughter binary branching, gen erate the category its non-head daughter .the model itself includes prior knowl spe cific ccg other than that only allows unary andbinary branching trees, and that the sets nontermi nals and terminals are not disjoint all the experiments reported this section were conducted using sections ccgbank training corpus, and section test corpus.the performance the baseline model shown the top row table for six out the sentences our test corpus not get parse.the reason that lexicon consisting the word category pairs observed the training corpus does not contain all the entries required parse the test corpus.']]\n",
      "____________________\n",
      "generative models statistical parsing combinatory categorial grammar paper compares number generative probability models wide-coverage combinatory categorial grammar parser. models trained tested corpus obtained translating penn treebank trees ccg normal-form derivations. rding evaluation unlabeled word-word dependencies, best model achieves performance .%, comparable figures given collins linguistically less expressivammar. contrast gildea find significant improvement modeling word-word dependencies. ccg combinatory rules encoded rule instances, together number additional rules deal punctuation type-changing. dependency features defined terms local rule instantiations, adding heads combining categories rule instantiation features.\n",
      "{'rouge1': Score(precision=0.38372093023255816, recall=0.11956521739130435, fmeasure=0.18232044198895028), 'rouge2': Score(precision=0.07058823529411765, recall=0.02181818181818182, fmeasure=0.03333333333333334), 'rougeL': Score(precision=0.1511627906976744, recall=0.04710144927536232, fmeasure=0.0718232044198895)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][4]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "print(summary2)\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][4]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a307ff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reproduced here for reference: given three nouns and only more recently has been suggested that corpus statistics might provide the oracle, and this idea the basis the three algorithms which use the adjacency model.', 'one problem with the training methods given section the restriction training data nouns many nouns, especially common ones, have verbal adjectival usages that preclude them from being yet when they occur nouns, they still provide useful training information that the current system ignores.', 'this result rdance with the informal reasoning given section the model also has the further commendation that predicts correctly the observed proportion left-branching compounds found two independently extracted test sets.', 'this result rdance with the informal reasoning given section the model also has the further commendation that predicts correctly the observed proportion left-branching compounds found two independently extracted test sets.']\n",
      "[['reproduced here for reference: given three nouns and only more recently has been suggested that corpus statistics might provide the oracle, and this idea the basis the three algorithms which use the adjacency model.one problem with the training methods given section the restriction training data nouns many nouns, especially common ones, have verbal adjectival usages that preclude them from being yet when they occur nouns, they still provide useful training information that the current system ignores.this result rdance with the informal reasoning given section the model also has the further commendation that predicts correctly the observed proportion left-branching compounds found two independently extracted test sets.this result rdance with the informal reasoning given section the model also has the further commendation that predicts correctly the observed proportion left-branching compounds found two independently extracted test sets.']]\n",
      "____________________\n",
      "corpus statistics meet noun compound: empirical results variety statistical methods noun compound analysis implemented compared. results support two main conclusions. first, use conceptual association enables broad coverage, also improves racy. second, analysis model based dependency grammar substantially rate one based deepest constituents, even though latter prevalent literature. propose unsupervised method estimating frequencies competing bracketings based taxonomy thesaurus. test adjacency dependency models compounds extracted grolier encyclopedia, corpus million words.\n",
      "{'rouge1': Score(precision=0.2647058823529412, recall=0.12949640287769784, fmeasure=0.17391304347826086), 'rouge2': Score(precision=0.014925373134328358, recall=0.007246376811594203, fmeasure=0.00975609756097561), 'rougeL': Score(precision=0.14705882352941177, recall=0.07194244604316546, fmeasure=0.0966183574879227)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][5]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "print(summary2)\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][5]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "34fc39a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the one hand, computational linguistics volume number used annotate what kay has called permanent predictable ambiguities, allowing annotator indicate that structure isglobally ambiguous even given the surrounding context example this use pseudo-attachment shown figure where the participial phrase blown ashore years ago modifies either warriors boatload, but there way settling the question both attachments mean exactly the same thing.', 'would like emphasize that the lexical and syntactic recoverability inherent the pos-tagged version the penn treebank corpus allows end users employ much richer tagset han the small one described section the need arises.', 'about million words pos-tagged material and small sam- piing skeletally parsed text are available part the first association for com- putational linguistics/data collection initiative cd-rom, and somewhat larger subset materials available cartridge tape directly from the penn treebank project.', 'reflects not only true mistakes parts performance, but also the many and important differences the usage penn treebank pos tags and the usage tags the original brown corpus material which parts was trained.']\n",
      "[['the one hand, computational linguistics volume number used annotate what kay has called permanent predictable ambiguities, allowing annotator indicate that structure isglobally ambiguous even given the surrounding context example this use pseudo-attachment shown figure where the participial phrase blown ashore years ago modifies either warriors boatload, but there way settling the question both attachments mean exactly the same thing.would like emphasize that the lexical and syntactic recoverability inherent the pos-tagged version the penn treebank corpus allows end users employ much richer tagset han the small one described section the need arises.about million words pos-tagged material and small sam- piing skeletally parsed text are available part the first association for com- putational linguistics/data collection initiative cd-rom, and somewhat larger subset materials available cartridge tape directly from the penn treebank project.reflects not only true mistakes parts performance, but also the many and important differences the usage penn treebank pos tags and the usage tags the original brown corpus material which parts was trained.']]\n",
      "____________________\n",
      "building large annotated corpus ish: penn treebank\n",
      "{'rouge1': Score(precision=0.5714285714285714, recall=0.023668639053254437, fmeasure=0.045454545454545456), 'rouge2': Score(precision=0.16666666666666666, recall=0.005952380952380952, fmeasure=0.011494252873563218), 'rougeL': Score(precision=0.5714285714285714, recall=0.023668639053254437, fmeasure=0.045454545454545456)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][6]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "print(summary2)\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][6]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1b10d5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['topic signatures can not only recognize related terms but grout related terms togetlmr under one target concept ibi iresent section pic: signatures can also shed some light the creation topic signatures.', 'the next section, describe the automated text smmnarization system summarist that used the experiments provide the context discussion.', 'experimental results order assess the utility topic signatures text sununarization, follow the procedure de- scribed the end section create topic signature for each selected trec topic.', 'the mnnber documents with answer keys are listed the row labeled: relevant does used training ensure utilize all the available data and conduct sound evaluation, perform three-fold eat three times with non- overlapl ing test set.']\n",
      "[['topic signatures can not only recognize related terms but grout related terms togetlmr under one target concept ibi iresent section pic: signatures can also shed some light the creation topic signatures.the next section, describe the automated text smmnarization system summarist that used the experiments provide the context discussion.experimental results order assess the utility topic signatures text sununarization, follow the procedure de- scribed the end section create topic signature for each selected trec topic.the mnnber documents with answer keys are listed the row labeled: relevant does used training ensure utilize all the available data and conduct sound evaluation, perform three-fold eat three times with non- overlapl ing test set.']]\n",
      "____________________\n",
      "automated acquisition topic signatures text summarization order produce good summary, one identify relevant portions given text. describe paper method automatically training topic signatures sets related words, associated wts, organized head topics illustrate signature created trec collection texts selected topics. describe possible integration topic signatures ontologies evaluaton automated text summarization system. first introduced topic signatures topic relevant terms summarization.\n",
      "{'rouge1': Score(precision=0.43103448275862066, recall=0.22321428571428573, fmeasure=0.29411764705882354), 'rouge2': Score(precision=0.10526315789473684, recall=0.05405405405405406, fmeasure=0.07142857142857144), 'rougeL': Score(precision=0.2413793103448276, recall=0.125, fmeasure=0.16470588235294117)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][7]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "print(summary2)\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][7]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cb0786a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also show that language pairs for which very little parallel data available are likely benefit the most from our method running our extraction system large comparable corpus bootstrapping manner, can obtain performance improvements more than over baseline system trained only existing parallel data.', 'there are relatively few language pairs for which parallel corpora reasonable sizes are available and even for those pairs, the corpora come mostly from one domain, that political discourse this especially problematic for the field statistical machine translation because translation systems trained data from particular domain will perform poorly when translating texts from different domain one way alleviate this lack parallel data exploit much more available and diverse resource: comparable non-parallel corpora.', 'two sentences may share many content words and yet express different meanings however, our task getting useful training data does not require perfect solution have seen, even such noisy training pairs can help improve translation system’s performance.', 'section evaluate the extracted data showing that adding out-of-domain parallel data improves the in-domain performance out-of-domain system, and section show that certain cases, even larger improvements can obtained using bootstrapping.']\n",
      "[['also show that language pairs for which very little parallel data available are likely benefit the most from our method running our extraction system large comparable corpus bootstrapping manner, can obtain performance improvements more than over baseline system trained only existing parallel data.there are relatively few language pairs for which parallel corpora reasonable sizes are available and even for those pairs, the corpora come mostly from one domain, that political discourse this especially problematic for the field statistical machine translation because translation systems trained data from particular domain will perform poorly when translating texts from different domain one way alleviate this lack parallel data exploit much more available and diverse resource: comparable non-parallel corpora.two sentences may share many content words and yet express different meanings however, our task getting useful training data does not require perfect solution have seen, even such noisy training pairs can help improve translation system’s performance.section evaluate the extracted data showing that adding out-of-domain parallel data improves the in-domain performance out-of-domain system, and section show that certain cases, even larger improvements can obtained using bootstrapping.']]\n",
      "____________________\n",
      "improving machine translation performance exploiting non-parallel corpora present novel method discovering parallel sentences comparable, non-parallel corpora. train maximum entropy classifier that, given pair sentences, reliably determine whether translations other. using approach, extract parallel data large chinese, arabic, ish non-parallel newspaper corpora. evaluate quality extracted data showing improves performance state-of-the-art statistical machine translation system. also show good-quality system built scratch starting small parallel corpus exploiting large non-parallel corpus. thus, method applied great benefit language pairs scarce resources available. use publication date vector-based similarity identify similar news articles. filter negative examples high difference low word overlap define features primarily based ibm model alignments\n",
      "{'rouge1': Score(precision=0.44545454545454544, recall=0.25925925925925924, fmeasure=0.3277591973244147), 'rouge2': Score(precision=0.11009174311926606, recall=0.06382978723404255, fmeasure=0.08080808080808081), 'rougeL': Score(precision=0.2, recall=0.1164021164021164, fmeasure=0.1471571906354515)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][8]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "print(summary2)\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][8]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d6d44e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['related study, they make claim that for given discourse, ambiguous words are used single sense with high probability leacock, chodorow, and miller chall this claim their work combining topical and local context, which shows that both topical and local context are required achieve consistent results across polysemous words text yarowsky study indicates that while information within large window can used disambiguate nouns, for verbs and adjectives the size the usable window drops off dramatically with distance from the target word.', 'recent work, syntactic information most often simply part speech, used invariably conjunction with other kinds information evidence suggests that different kinds disambiguation procedures are needed depending the syntactic category and other characteristics the target word —an idea reminiscent the word expert approach.', 'step the assignment words senses, mplished reliance two major sources information: all disambiguation work involves matching the context the instance the word disambiguated with either information from external knowl source information about the contexts previously disambiguated instances the word derived from corpora any variety association methods used determine the best match between the current context and one these sources information, order assign sense each word occurrence.', 'experiments ambiguous words, each six contexts produced correct disambiguation, using the relatively fine sense distinctions the ced, the cases later experiments, improving the parameters and only distinguishing homographs enabled rate applied the task mapping the senses the ced and oald for the same words this method obtained correct correspondence the cases the sense level, and the level homographs sutcliffe and slater replicated this method full text and found similar results several authors have attempted improve results using supplementary fields information the electronic version the longman dictionary contemporary ish particular, the box codes and subject codes provided for each sense.']\n",
      "[['related study, they make claim that for given discourse, ambiguous words are used single sense with high probability leacock, chodorow, and miller chall this claim their work combining topical and local context, which shows that both topical and local context are required achieve consistent results across polysemous words text yarowsky study indicates that while information within large window can used disambiguate nouns, for verbs and adjectives the size the usable window drops off dramatically with distance from the target word.recent work, syntactic information most often simply part speech, used invariably conjunction with other kinds information evidence suggests that different kinds disambiguation procedures are needed depending the syntactic category and other characteristics the target word —an idea reminiscent the word expert approach.step the assignment words senses, mplished reliance two major sources information: all disambiguation work involves matching the context the instance the word disambiguated with either information from external knowl source information about the contexts previously disambiguated instances the word derived from corpora any variety association methods used determine the best match between the current context and one these sources information, order assign sense each word occurrence.experiments ambiguous words, each six contexts produced correct disambiguation, using the relatively fine sense distinctions the ced, the cases later experiments, improving the parameters and only distinguishing homographs enabled rate applied the task mapping the senses the ced and oald for the same words this method obtained correct correspondence the cases the sense level, and the level homographs sutcliffe and slater replicated this method full text and found similar results several authors have attempted improve results using supplementary fields information the electronic version the longman dictionary contemporary ish particular, the box codes and subject codes provided for each sense.']]\n",
      "____________________\n",
      "introduction special issue word sense disambiguation: state art present concise survey history ideas used word sense disambiguation. general, various wsd approaches divided two types, data knowl-based approaches. argue word sense ambiguity central problem many established hlt applications\n",
      "{'rouge1': Score(precision=0.3684210526315789, recall=0.04878048780487805, fmeasure=0.08615384615384616), 'rouge2': Score(precision=0.02702702702702703, recall=0.0034965034965034965, fmeasure=0.006191950464396285), 'rougeL': Score(precision=0.2894736842105263, recall=0.03832752613240418, fmeasure=0.06769230769230768)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][9]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][9]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "96dfc809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['interesting connection has been suggested direct parsing for id/lp grammars which word-order variations would mmodated the parser, and related ideas for generation free word-order languages the tag framework our work differs from the id/lp work several important respects.the earlier system, translation single sentences required the order hours contrast the new algorithm generally takes less than one minute—usually substantially less—with special optimization the code.the earlier system, translation single sentences required the order hours contrast the new algorithm generally takes less than one minute—usually substantially less—with special optimization the code.first approach improving the translation search limit the allowed word alignment patterns those permitted btg.']]\n",
      "____________________\n",
      "polynomial-time algorithm statistical machine translation introduce polynomial-time algorithm statistical machine translation. algorithm used place expensive, slow best-first search strategies current statistical translation architectures. approach employs stochastic bracketing transduction grammar model recently introduced replace earlier word alignment channel models, retaining bigram language model. new algorithm experience yields major speed improvement significant loss racy. test algorithm chinese-ish translation.\n",
      "{'rouge1': Score(precision=0.25, recall=0.13274336283185842, fmeasure=0.17341040462427748), 'rouge2': Score(precision=0.03389830508474576, recall=0.017857142857142856, fmeasure=0.023391812865497075), 'rougeL': Score(precision=0.13333333333333333, recall=0.07079646017699115, fmeasure=0.09248554913294797)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][15]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][15]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a066dac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['interestingly, nps are also used anchor points learn monolingual paraphrases the phrasal structure categories can extracted from automatic parsers using methods illustration given below the placement the dependency arcs reflects the relative word order between parent node and all its immediate children.all these approaches, though different formalism, model the two languages using tree-based transduction rules synchronous grammar, possibly probabilistic, and using multi-lemma elementary structures atomic units.analogous the transition probability bringing equations together, the best translation would maximize: observing the similarity between our model and hmm, our dynamic programming decoding algorithm spirit similar the viterbi algorithm except that instead being sequential the decoding done trees top down fashion.while statistical modeling children reordering one possible remedy for this problem, believe simple linguistic treatment another, the output the sdig system ish dependency tree rather than string words.']]\n",
      "____________________\n",
      "machine translation using probabilistic synchronous dependency insertion grammars syntax-based statistical machine translation aims applying statistical models structured data. paper, present syntax-based statistical machine translation system based probabilistic synchronous dependency insertion grammar. synchronous dependency insertion grammars version synchronous grammars defined dependency trees. first introduce approach inducing grammar parallel corpora. second, describe thaphical model machine translation task, also viewed stochastic tree-to-tree transducer. introduce polynomial time decoding algorithm model. evaluate outputs system using nist bleu automatic evaluation software. result shows system outperforms baseline system based ibm models translation speed quality. approach requires assumptions level isomorphism two languages. present translation model based synchronous dependency insertion grammar handles non-isomorphism requires source target dependency structures.\n",
      "{'rouge1': Score(precision=0.23684210526315788, recall=0.19424460431654678, fmeasure=0.21343873517786563), 'rouge2': Score(precision=0.04424778761061947, recall=0.036231884057971016, fmeasure=0.0398406374501992), 'rougeL': Score(precision=0.11403508771929824, recall=0.09352517985611511, fmeasure=0.10276679841897234)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][20]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][20]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8837d869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['then calculated the racy counting the number words whose top one candidate identical itself, obtaining precision allowing n-top candidates, the racy improves shown thaphs for words output figure find the correct translation among the top candidates, obtain precision around n-top candidates are useful translator aids.early work uses pair non-parallel texts for the task lexical disambiguation between several senses word this basic idea extends choosing translation among multiple candidates given collocation information.evaluation shows precision about showed that humans are able translate more than twice many japanese technical terms into ish when our system output used, compared translating random set japanese terms without aid.addition the evaluation results, have also discovered that the content words the same segment with word term all contribute the occurrence this word.']]\n",
      "____________________\n",
      "finding terminology translations non-parallel corpora present statistical word feature, word relation matrix, used find translated pairs words terms non-parallel corpora, languagoups. online dictionary entries used seed words generate word relation matrices unknown words rding correlation measures. word relation matrices mapped corpora find translation pairs. translation racies top candidate counted. nevertheless, top candidate output give average increase racy human translator performance. work, translation model applied pair unrelated languages random selection test words, many multi-word terms, gives precision top candidate proposed.\n",
      "{'rouge1': Score(precision=0.4146341463414634, recall=0.26153846153846155, fmeasure=0.32075471698113206), 'rouge2': Score(precision=0.06172839506172839, recall=0.03875968992248062, fmeasure=0.047619047619047616), 'rougeL': Score(precision=0.2073170731707317, recall=0.13076923076923078, fmeasure=0.16037735849056603)}\n"
     ]
    }
   ],
   "source": [
    "text = df[\"cleaned_text\"][50]\n",
    "sentences = []\n",
    "sentences.append(sent_tokenize(text))\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "clean_sentences = pd.Series(sentences)\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "    #clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "        \n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    #Summary Extraction\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "summary2 = []\n",
    "for i in range(4):\n",
    "    summary2.append(ranked_sentences[i][1])\n",
    "        #summary2 += ranked_sentences[i][1]\n",
    "allsum2 = []\n",
    "allsum2.append(summary2)\n",
    "reference1 = df['cleaned_summary'][50]\n",
    "hypotheses1 = allsum2\n",
    "hypotheses1 = str(hypotheses1)\n",
    "hypotheses1 = re.sub(\"', '\", '', hypotheses1)\n",
    "reference1 = str(reference1)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores1 = scorer.score(hypotheses1,\n",
    "                      reference1)\n",
    "print(hypotheses1)\n",
    "print(\"____________________\")\n",
    "print(reference1)\n",
    "print(scores1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
