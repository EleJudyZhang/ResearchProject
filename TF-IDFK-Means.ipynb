{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c04b1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469c786a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TnT - A Statistical Part-Of-Speech Tagger Trig...</td>\n",
       "      <td>TnT - A Statistical Part-Of-Speech Tagger\\nTri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mildly Non-Projective Dependency Structures Sy...</td>\n",
       "      <td>Mildly Non-Projective Dependency Structures\\nS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Using Corpus Statistics And WordNet Relations ...</td>\n",
       "      <td>Using Corpus Statistics And WordNet Relations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Automatic Labeling Of Semantic Roles present a...</td>\n",
       "      <td>Automatic Labeling Of Semantic Roles\\nWe prese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generative Models For Statistical Parsing With...</td>\n",
       "      <td>Generative Models For Statistical Parsing With...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  TnT - A Statistical Part-Of-Speech Tagger Trig...   \n",
       "1  Mildly Non-Projective Dependency Structures Sy...   \n",
       "2  Using Corpus Statistics And WordNet Relations ...   \n",
       "3  Automatic Labeling Of Semantic Roles present a...   \n",
       "4  Generative Models For Statistical Parsing With...   \n",
       "\n",
       "                                             summary  \n",
       "0  TnT - A Statistical Part-Of-Speech Tagger\\nTri...  \n",
       "1  Mildly Non-Projective Dependency Structures\\nS...  \n",
       "2  Using Corpus Statistics And WordNet Relations ...  \n",
       "3  Automatic Labeling Of Semantic Roles\\nWe prese...  \n",
       "4  Generative Models For Statistical Parsing With...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"scisumm.csv\"\n",
    "                 ,nrows=201)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "505669e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def TextCleaning(text,num):\n",
    "    text = text.lower()\n",
    "    #cleantext = re.sub(\"\\(.*?\\)\", '', text)\n",
    "    cleantext = re.sub(\"[0-9]\", '', text)\n",
    "    cleantext = re.sub(\"(\\.\\.+)\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"(--+)\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"(~~+)\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"[<>()|&©ø\\[\\]\\'\\\";~*]\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"(\\+\\++)\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"(__+)\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"e.g.\", '',cleantext)\n",
    "    cleantext = re.sub(\"i.e.,\", '',cleantext)\n",
    "    cleantext = re.sub(\"acc.\", '',cleantext)\n",
    "    #cleantext = re.sub(\"[^a-zA-Z]\", ' ',cleantext)\n",
    "    cleantext = re.sub(\"(\\s+)\",' ',cleantext)\n",
    "    if(num==0):\n",
    "        tokens = [w for w in cleantext.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=cleantext.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>2:                                              \n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71b6915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tnt statistical part-of-speech tagger trigrams...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text\n",
       "0  tnt statistical part-of-speech tagger trigrams..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = []\n",
    "for t in data['text']:\n",
    "    cleaned_text.append(TextCleaning(t,2)) \n",
    "#call the function\n",
    "cleaned_summary = []\n",
    "for t in data['summary']:\n",
    "    cleaned_summary.append(TextCleaning(t,0))\n",
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary\n",
    "data.dropna(axis=0,inplace=True)\n",
    "data.iloc[[0],[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7da9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data set and parameters\n",
    "raw_data = data['cleaned_text'][0]\n",
    "ps = PorterStemmer()\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "cluster_count = 8\n",
    "\n",
    "sentences = sent_tokenize(raw_data)\n",
    "processedSentences = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1fae21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the suffix strong predictor for word classes, words the wall street journal part the penn treebank ending able are adjectives the cases fashionable, variable the rest are nouns cable, variable the probability distribution for particular suffix generated from all words the training set that share the same suffix some predefined maximum lh. use context-independent approach for did for the contextual wts turned out good choice set all the standard deviation the unconditioned maximum likelihood probabilities the tags the training corpus, set for all using tagset tags and the average this usually yields values the range use different estimates for uppercase and lowercase words, maintain two different suffix tries depending the capitalization the word. the tests are performed partitions the corpora that use training set and test set, that the test data guaranteed unseen during training. tagging racies for the negra corpus are shown table figure shows the learning curve the tagger, the racy depending the amount training data. the training given logarithmic scale. exploit the fact that the tagger not only determines tags, but also assigns probabilities. the annotation consists four parts: context-free structure augmented with traces mark movement and discontinuous constituents, phrasal categories that are annotated node labels, small set grammatical functions that are annotated extensions the node labels, and part-of-speech tags marcus al., this evaluation only uses the part-ofspeech annotation. figure shows the racy when separating assignments with quotients larger and smaller than the threshold hence reliable and unreliable assignments again, find that racies for reliable assignments are much higher than for unreliable assignments. \n"
     ]
    }
   ],
   "source": [
    "# create tfidf matrix from the processed sentences\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processedSentences)\n",
    "\n",
    "# cluster our tokenized sentences into 10 groups\n",
    "kMeansCluster = KMeans(n_clusters=cluster_count)\n",
    "kMeansCluster.fit(tfidf_matrix)\n",
    "kmeansmodel = kMeansCluster.fit(tfidf_matrix)\n",
    "clusters = kMeansCluster.labels_.tolist()\n",
    "\n",
    "\n",
    "sentenceDictionary = {}\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    sentenceDictionary[idx] = {}\n",
    "\n",
    "    sentenceDictionary[idx]['cluster'] = clusters[idx]\n",
    "    sentenceDictionary[idx]['stemmed'] = processedSentences[idx]\n",
    "\n",
    "# Create new dictionary that contains 1 entry for each cluster\n",
    "# each key in dictionary will point to array of sentences, all of which belong to that cluster\n",
    "# we attach the index to the sentenceDictionary object so we can recall the original sentence\n",
    "clusterDictionary = {}\n",
    "for key, sentence in sentenceDictionary.items():\n",
    "    if sentence['cluster'] not in clusterDictionary:\n",
    "        clusterDictionary[sentence['cluster']] = []\n",
    "    clusterDictionary[sentence['cluster']].append(sentence['stemmed'])\n",
    "    sentence['idx'] = len(clusterDictionary[sentence['cluster']]) - 1\n",
    "\n",
    "\n",
    "# For each cluster of sentences,\n",
    "# Find the sentence with highet cosine similarity over all sentences in cluster\n",
    "maxCosineScores = {}\n",
    "for key, clusterSentences in clusterDictionary.items():\n",
    "    maxCosineScores[key] = {}\n",
    "    maxCosineScores[key]['score'] = 0\n",
    "    tfidf_matrix = vectorizer.fit_transform(clusterSentences)\n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    for idx, row in enumerate(cos_sim_matrix):\n",
    "        sum = 0\n",
    "        for col in row:\n",
    "            sum += col\n",
    "        if sum > maxCosineScores[key]['score']:\n",
    "            maxCosineScores[key]['score'] = sum\n",
    "            maxCosineScores[key]['idx'] = idx\n",
    "\n",
    "# for every cluster's max cosine score,\n",
    "# find the corresponding original sentence\n",
    "resultIndices = []\n",
    "i = 0\n",
    "for key, value in maxCosineScores.items():\n",
    "    cluster = key\n",
    "    idx = value['idx']\n",
    "    stemmedSentence = clusterDictionary[cluster][idx]\n",
    "# key corresponds to the sentences index of the original document\n",
    "# we will use this key to sort our results in order of original document\n",
    "    for key, value in sentenceDictionary.items():\n",
    "        if value['cluster'] == cluster and value['idx'] == idx:\n",
    "            resultIndices.append(key)\n",
    "\n",
    "resultIndices.sort()\n",
    "\n",
    "# Iterate over sentences and construct summary output\n",
    "result = ''\n",
    "for idx in resultIndices:\n",
    "    result += sentences[idx] + ' '\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bed93f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additionally, present results the tagger the negra corpus brants al., and the penn treebank marcus al., the penn treebank results reported here for the markov model approach are least equivalent those reported for the maximum entropy approach ratnaparkhi, for comparison other taggers, the reader referred zavrel and daelemans, tnt uses second order markov models for part-ofspeech tagging. define maximum likelihood probability zero the corresponding nominators and denominators are zero. use the context-independent variant linear interpolation, the values the not depend the particular trigram. the suffix strong predictor for word classes, words the wall street journal part the penn treebank ending able are adjectives the cases fashionable, variable the rest are nouns cable, variable the probability distribution for particular suffix generated from all words the training set that share the same suffix some predefined maximum lh. training the number tokens used for training. exploit the fact that the tagger not only determines tags, but also assigns probabilities. the table shows the percentage unknown tokens, separate racies and standard deviations for known and unknown tokens, well the overall racy. large annotated corpora are the pre-requisite for developing and testing part-ofspeech taggers, and they enable thneration high-quality language models. \n",
      "____________________\n",
      "tnt statistical part-of-speech tagger trigrams tags tnt efficient statistical part-of-speech tagger. contrary claims found elsewhere literature, argue tagger based markov models performs least well current approaches, including maximum entropy framework. recent comparison even shown tnt performs significantly better tested corpora. describe basic model tnt, techniques used smoothing handling unknown words. furthermore, present evaluations two corpora. achieve automated tagging syntactic-structure-based set grammatical function tags including phrase-chunk syntactic-role modifiers trained supervised mode tree bank german.\n",
      "{'rouge1': Score(precision=0.32098765432098764, recall=0.12935323383084577, fmeasure=0.1843971631205674), 'rouge2': Score(precision=0.025, recall=0.01, fmeasure=0.014285714285714285), 'rougeL': Score(precision=0.16049382716049382, recall=0.06467661691542288, fmeasure=0.0921985815602837)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference = data.loc[0,'cleaned_summary']\n",
    "\n",
    "hypotheses = str(result)\n",
    "reference = str(reference)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(hypotheses,\n",
    "                      reference)\n",
    "print(hypotheses)\n",
    "print(\"____________________\")\n",
    "print(reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bffc29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "allsum4 = []\n",
    "for text in data['cleaned_text']:\n",
    "    raw_data = text\n",
    "    ps = PorterStemmer()\n",
    "    nltk_stop_words = set(stopwords.words('english'))\n",
    "    cluster_count = 3\n",
    "    min_sentence_length = 30\n",
    "    sentences = sent_tokenize(raw_data)\n",
    "    processedSentences = sentences\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processedSentences)\n",
    "\n",
    "\n",
    "    kMeansCluster = KMeans(n_clusters=cluster_count)\n",
    "    kMeansCluster.fit(tfidf_matrix)\n",
    "    clusters = kMeansCluster.labels_.tolist()\n",
    "\n",
    "\n",
    "    sentenceDictionary = {}\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        sentenceDictionary[idx] = {}\n",
    "        sentenceDictionary[idx]['text'] = sentence\n",
    "        sentenceDictionary[idx]['cluster'] = clusters[idx]\n",
    "        sentenceDictionary[idx]['stemmed'] = processedSentences[idx]\n",
    "        \n",
    "    clusterDictionary = {}\n",
    "    for key, sentence in sentenceDictionary.items():\n",
    "        if sentence['cluster'] not in clusterDictionary:\n",
    "            clusterDictionary[sentence['cluster']] = []\n",
    "        clusterDictionary[sentence['cluster']].append(sentence['stemmed'])\n",
    "        sentence['idx'] = len(clusterDictionary[sentence['cluster']]) - 1\n",
    "        \n",
    "    maxCosineScores = {}\n",
    "    for key, clusterSentences in clusterDictionary.items():\n",
    "        maxCosineScores[key] = {}\n",
    "        maxCosineScores[key]['score'] = 0\n",
    "        tfidf_matrix = vectorizer.fit_transform(clusterSentences)\n",
    "        cos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "        for idx, row in enumerate(cos_sim_matrix):\n",
    "            sum = 0\n",
    "            for col in row:\n",
    "                sum += col\n",
    "            if sum > maxCosineScores[key]['score']:\n",
    "                maxCosineScores[key]['score'] = sum\n",
    "                maxCosineScores[key]['idx'] = idx\n",
    "\n",
    "    resultIndices = []\n",
    "    i = 0\n",
    "    for key, value in maxCosineScores.items():\n",
    "        cluster = key\n",
    "        idx = value['idx']\n",
    "        stemmedSentence = clusterDictionary[cluster][idx]\n",
    "        for key, value in sentenceDictionary.items():\n",
    "            if value['cluster'] == cluster and value['idx'] == idx:\n",
    "                resultIndices.append(key)\n",
    "\n",
    "    resultIndices.sort()\n",
    "    \n",
    "    result = ''\n",
    "    for idx in resultIndices:\n",
    "        result += sentences[idx] + ' '\n",
    "    allsum4.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e33d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"result\"] = allsum4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e02b967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      first step, use the maximum likelihood probabi...\n",
      "1      section provide experimental evaluation the no...\n",
      "2      one should not conclude from these results tha...\n",
      "3      given input sentence and target word frame, th...\n",
      "4      sult, the categorial lexicon extracted for thi...\n",
      "                             ...                        \n",
      "196    explicitly representing these two steps indepe...\n",
      "197    limitations co-training for natural language l...\n",
      "198    while this result encouraging, seems that the ...\n",
      "199    known that holds and only the function satisfi...\n",
      "200    reordering approaches havven significant impro...\n",
      "Name: result, Length: 201, dtype: object\n",
      "____________________\n",
      "0      tnt statistical part-of-speech tagger trigrams...\n",
      "1      mildly non-projective dependency structures sy...\n",
      "2      using corpus statistics wordnet relations sens...\n",
      "3      automatic labeling semantic roles present syst...\n",
      "4      generative models statistical parsing combinat...\n",
      "                             ...                        \n",
      "196    applying co-training methods statistical parsi...\n",
      "197    limitations co-training natural language learn...\n",
      "198    ish all-words task describe experience prepari...\n",
      "199    japanese dependency structure analysis based s...\n",
      "200    chinese syntactic reordering statistical machi...\n",
      "Name: cleaned_summary, Length: 201, dtype: object\n",
      "{'rouge1': Score(precision=0.2987012987012987, recall=0.27058823529411763, fmeasure=0.28395061728395066), 'rouge2': Score(precision=0.09210526315789473, recall=0.08333333333333333, fmeasure=0.0875), 'rougeL': Score(precision=0.2727272727272727, recall=0.24705882352941178, fmeasure=0.2592592592592593)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference = data['cleaned_summary']\n",
    "hypotheses = data[\"result\"]\n",
    "\n",
    "hypotheses = str(hypotheses)\n",
    "reference = str(reference)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(hypotheses,\n",
    "                      reference)\n",
    "print(hypotheses)\n",
    "print(\"____________________\")\n",
    "print(reference)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
